{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K1BJnwg1oDrD",
        "oJJbgw9erL3b",
        "m2IYFccNgtnP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install required libraries**"
      ],
      "metadata": {
        "id": "K1BJnwg1oDrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuPa3OVXMJ-",
        "outputId": "820de284-751f-41be-c7a8-f50285fdd278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/817.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/817.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m737.3/817.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.51-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.46 langchain-text-splitters-0.0.1 langsmith-0.1.51 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8663a8b-038f-4744-8e7c-16d89d1cfda6",
        "id": "-1tY7JZto_fm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIzaSyDrvCqq0NeI2ZcOxSpe9aqx6qC6IK91eFU"
      ],
      "metadata": {
        "id": "GcE6nZSGpErz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3390bdb4-4274-4304-d2fe-84e60a3c2504",
        "id": "bLbJwOSQo_fm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your Google API Key··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "p-LssKDkodI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining Gemini Model**"
      ],
      "metadata": {
        "id": "oJJbgw9erL3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
      ],
      "metadata": {
        "id": "oq74kMRWrOFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "werwRR6_rPQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating prompt templates**"
      ],
      "metadata": {
        "id": "Zb1oERjOZd7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "k6YYANd8pdDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt template main**"
      ],
      "metadata": {
        "id": "pqx2KxaJprPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt template for extracting names**"
      ],
      "metadata": {
        "id": "G6u5snwtogc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_string_main = \"\"\" go through this meticulously: '{text}'. Based on what you went through, return one of the following tasks to do: 1)Generate name ,2)Summarize ,3)Other\"\"\"\n",
        "prompt_template_main = ChatPromptTemplate.from_template(prompt_string_main)"
      ],
      "metadata": {
        "id": "v1SqAX5Hpk5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_extract_names = \"\"\" Extract only the names from the following text : ['{text}']. Return the names one after the other. \"\"\"\n",
        "prompt_template_extract_names = ChatPromptTemplate.from_template(prompt_extract_names)"
      ],
      "metadata": {
        "id": "p0Sd_gxTp58M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt template for summarization**"
      ],
      "metadata": {
        "id": "mGI6edFqfCve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_summary = \"\"\" Summarize the following : '{text}'\"\"\"\n",
        "prompt_template_summary = ChatPromptTemplate.from_template(prompt_summary)"
      ],
      "metadata": {
        "id": "gBWxA_A6p3GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt template for extracting top 20 viable human names from all predictions**"
      ],
      "metadata": {
        "id": "Cf8t0JY_x5lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_string_viable_names = \"\"\"Your task is to identify only the best 20 viable human names from the following strings:\n",
        "[{text}]. Return the identified names one after the other. Do not number them. Do not return more than 20 names. \"\"\"\n",
        "prompt_template_viable_names = ChatPromptTemplate.from_template(prompt_string_viable_names)"
      ],
      "metadata": {
        "id": "p5qpNPi_Zyc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "c5XPu9BFAGpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing the first and second step of pipeline - Is the model able to differentiate what task to perform based on user prompt**"
      ],
      "metadata": {
        "id": "m2IYFccNgtnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_input1=\"\"\"I need you to help me summarize a piece of text\"\"\"\n",
        "main_input2=\"\"\"I need to come up with a unique name \"\"\"\n",
        "main_input3=\"\"\"I need to translate a piece of text\"\"\""
      ],
      "metadata": {
        "id": "fmV8pNYaq0DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_task_prompt = prompt_template_main.format_messages(text=main_input1)\n",
        "main_task2_prompt = prompt_template_main.format_messages(text=main_input2)\n",
        "main_task3_prompt = prompt_template_main.format_messages(text=main_input3)"
      ],
      "metadata": {
        "id": "uBmMjTm6q0DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "9GXuZbKhrEuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_task = Gemini.invoke(main_task_prompt)\n",
        "print(main_task.content)\n",
        "\n",
        "main_task2 = Gemini.invoke(main_task2_prompt)\n",
        "print(main_task2.content)\n",
        "\n",
        "main_task3 = Gemini.invoke(main_task3_prompt)\n",
        "print(main_task3.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581122a8-9a93-41ea-ac30-39e4de2bbda7",
        "id": "3kVL5RT1rEuW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2)Summarize\n",
            "1) Generate name\n",
            "3)Other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the required functions and data for the Third step of the pipeline - generate output for the prompts**"
      ],
      "metadata": {
        "id": "NWOCvpsfiaEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **loading the character mappings**"
      ],
      "metadata": {
        "id": "KkqE4pc_s7ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NLM/Wave/Mappings/itos_bigdataset.pkl'\n",
        "\n",
        "# Load the dictionary from the file\n",
        "with open(file_path, 'rb') as file:\n",
        "    itos = pickle.load(file)\n",
        "\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn6n-oWzcOFe",
        "outputId": "8974a84e-de4a-4033-ca13-09833a0d1615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NLM/Wave/Mappings/stoi_bigdataset.pkl'\n",
        "\n",
        "# Load the dictionary from the file\n",
        "with open(file_path, 'rb') as file:\n",
        "    stoi = pickle.load(file)\n",
        "\n",
        "print(stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cVqbPchc4Zu",
        "outputId": "ac65b1c1-c63e-4974-fe3b-cc1e8f23c558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the model layers for the pretrained name generating model**\n",
        "\n",
        "### The model is a pretrained sequential object of layers, therefore requires the definition of each layer in the sequential object"
      ],
      "metadata": {
        "id": "J3tBQeI1tbW6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae93009e-66ba-49e7-a22c-97fb341459b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      if x.ndim == 2:\n",
        "        dim = 0\n",
        "      elif x.ndim == 3:\n",
        "        dim = (0,1)\n",
        "      xmean = x.mean(dim, keepdim=True)\n",
        "      xvar = x.var(dim, keepdim=True)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Embedding:\n",
        "\n",
        "  def __init__(self, num_embeddings, embedding_dim):\n",
        "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
        "\n",
        "  def __call__(self, IX):\n",
        "    self.out = self.weight[IX]\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight]\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class FlattenConsecutive:\n",
        "\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    x = x.view(B, T//self.n, C*self.n)\n",
        "    if x.shape[1] == 1:\n",
        "      x = x.squeeze(1)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Sequential:\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [p for layer in self.layers for p in layer.parameters()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load the pre trained name generating model**"
      ],
      "metadata": {
        "id": "e2_wc9MCuDRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/NLM/Wave/Final_Model_bigdataset/model3.pth')\n",
        "parameters = model.parameters()\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "Qy28mGVVP2nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load the existing names from dataset for comparison, to prevent generation of existing names**"
      ],
      "metadata": {
        "id": "3HNAj7I6u3XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "csv_file = '/content/drive/MyDrive/NLM/Gender_Data.csv'\n",
        "\n",
        "names_data = []\n",
        "\n",
        "with open(csv_file, 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        names_data.append(row[0])\n",
        "\n",
        "names=names_data[1:]\n",
        "print(len(names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrS5-dOXvMwB",
        "outputId": "40839379-4c91-4cf0-ba65-27bd4e470288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function to generate new names**"
      ],
      "metadata": {
        "id": "xF-o7A_hyeni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gka0o5hwP-rI"
      },
      "outputs": [],
      "source": [
        "block_size=16\n",
        "def get_names(model,names):\n",
        "  predicted=[]\n",
        "  while len(predicted)<100:\n",
        "\n",
        "      out = []\n",
        "      context = [0] * block_size\n",
        "      while True:\n",
        "        logits = model(torch.tensor([context]))\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        ix = torch.multinomial(probs, num_samples=1).item()\n",
        "        context = context[1:] + [ix]\n",
        "        out.append(ix)\n",
        "        if ix == 0:\n",
        "          break\n",
        "\n",
        "      name=''.join(itos[out[i]] for i in range(len(out)-1))\n",
        "      if name not in names:\n",
        "        predicted.append(name)\n",
        "  return predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Functions to fine tune on user entered names**"
      ],
      "metadata": {
        "id": "2nigza58ygtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n"
      ],
      "metadata": {
        "id": "gu-PgKiazLv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(model,user_entered_names):\n",
        "  parameters=model.parameters()\n",
        "  # Freeze all layers except last layer\n",
        "  for i in range(len(parameters)-1):\n",
        "    parameters[i].requires_grad=False\n",
        "\n",
        "\n",
        "  Xtr,Ytr=build_dataset(user_entered_names)\n",
        "\n",
        "  max_steps = 40000\n",
        "  batch_size = 32\n",
        "  lossi = []\n",
        "  lossi_val = []\n",
        "  min_loss=10\n",
        "\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    #calculate training loss\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "\n",
        "    logits = model(Xb)\n",
        "    loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "\n",
        "    #update weights based on training loss\n",
        "    loss.backward()\n",
        "\n",
        "    # lr = 0.1 if i < 150000 else 0.01\n",
        "    lr=0.1\n",
        "    for p in parameters:\n",
        "      if p.requires_grad:\n",
        "        p.data += -lr * p.grad\n",
        "\n",
        "    if i % 2000 == 0:\n",
        "\n",
        "\n",
        "      logits_train = model(Xtr)\n",
        "      loss_train = F.cross_entropy(logits_train, Ytr)\n",
        "\n",
        "\n",
        "      print(int(i/2000),\" epochs complete\")\n",
        "      print(f'Training loss : {loss_train.item():.4f} ')\n",
        "\n",
        "      if loss_train.item()<min_loss:\n",
        "        torch.save(model, '/content/finetune_model.pth')\n",
        "        min_loss=loss_train.item()\n",
        "\n",
        "\n",
        "  print(\"best model has been saved at '/content/finetune_model.pth'\")"
      ],
      "metadata": {
        "id": "j8eilCClyv6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the main function of the third step of the pipeline**"
      ],
      "metadata": {
        "id": "bI2UVraa09Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Task(task,user_input):\n",
        "  if 'name' in task.content.lower() or '1' in task.content.lower():\n",
        "    name_prompt = prompt_template_extract_names.format_messages(text=user_input)\n",
        "    all_names = Gemini.invoke(name_prompt[0].content).content\n",
        "\n",
        "    all_names=all_names.split('\\n')\n",
        "    if len(all_names)==1:\n",
        "      all_names=all_names[0].split(', ')\n",
        "\n",
        "    for i in range(len(all_names)):\n",
        "      all_names[i]=all_names[i].lower()\n",
        "\n",
        "    # Predict new names:\n",
        "    predicted_names=get_names(model,names)\n",
        "\n",
        "    with open(\"new_names.txt\", \"w\") as file:\n",
        "      for name in predicted_names:\n",
        "          file.write(str(name) + \"\\n\")\n",
        "\n",
        "    print(\"top 100 predictions new names are stored in /content/new_names.txt\")\n",
        "\n",
        "    names_string= ', '.join(predicted_names)\n",
        "    viable_names_prompt = prompt_template_viable_names.format_messages(text=names_string)\n",
        "    viable_names = Gemini.invoke(viable_names_prompt[0].content).content\n",
        "\n",
        "    viable_names=viable_names.split()\n",
        "\n",
        "\n",
        "    with open(\"new_names_viable.txt\", \"w\") as file:\n",
        "      for name in viable_names:\n",
        "          file.write(str(name) + \"\\n\")\n",
        "    print(\"top 20 viable human name predictions are stored in /content/new_names_viable.txt\")\n",
        "\n",
        "    print(\"\\n \\n\")\n",
        "\n",
        "\n",
        "\n",
        "    # Fine tuning on user entered names\n",
        "    print(\"Fine tuning on the following data : \",all_names)\n",
        "    fine_tune(model,all_names)\n",
        "    fine_tuned_model=torch.load('/content/finetune_model.pth')\n",
        "\n",
        "    for layer in fine_tuned_model.layers:\n",
        "      layer.training = False\n",
        "\n",
        "\n",
        "    print(\"\\n \\n\")\n",
        "\n",
        "\n",
        "    # Generating names after fine tuning\n",
        "    predicted_names_finetune=get_names(fine_tuned_model,names+all_names)\n",
        "\n",
        "    with open(\"new_names_finetune.txt\", \"w\") as file:\n",
        "      for name in predicted_names_finetune:\n",
        "          file.write(str(name) + \"\\n\")\n",
        "\n",
        "    print(\"top 100 predicted new names after fine tuning are stored in /content/new_names_finetune.txt\")\n",
        "\n",
        "    names_string= ', '.join(predicted_names_finetune)\n",
        "    viable_names_prompt = prompt_template_viable_names.format_messages(text=names_string)\n",
        "    viable_names = Gemini.invoke(viable_names_prompt[0].content).content\n",
        "\n",
        "    viable_names=viable_names.split()\n",
        "\n",
        "    with open(\"new_names_viable_finetune.txt\", \"w\") as file:\n",
        "      for name in viable_names:\n",
        "          file.write(str(name) + \"\\n\")\n",
        "    print(\"top 20 viable human name predictions after fine tuning are stored in /content/new_names_viable_finetune.txt\")\n",
        "\n",
        "\n",
        "  elif 'summarize' in task.content.lower() or '2' in task.content.lower():\n",
        "    summary_prompt=prompt_template_summary.format_messages(text=user_input)\n",
        "    summary=Gemini.invoke(summary_prompt[0].content)\n",
        "    print(\"Summary is : \")\n",
        "    print(summary.content)\n",
        "\n",
        "  else:\n",
        "    print(\"other\")\n"
      ],
      "metadata": {
        "id": "T8tyY_YQRXj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "JCOFjVVq1Qr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing the third step of the pipeline**"
      ],
      "metadata": {
        "id": "UtUWzvGy1LkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input=\"\"\"Hugging Face: Revolutionizing Natural Language Processing Introduction In the rapidly evolving field of Natural Language Processing (NLP),\n",
        "Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that\n",
        "has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark\n",
        "on the industry.  The Birth of Hugging Face Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf.\n",
        "The name Hugging Face was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug.\n",
        "Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
        "Transformative Innovations Hugging Face is best known for its open-source contributions, particularly the Transformers library.\n",
        "This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art\n",
        "pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language\n",
        "translation and sentiment analysis.\"\"\"\n",
        "\n",
        "\n",
        "user_input2=\"\"\"My name is Avaneesh and this is my family history. My father's name is Sundararajan , my mother's name is Rajeshwari , my brother's name is Aditya and my grandfather's name is Balasubramanian. My gradmother's name is Umadevi. My aunt's name is Rukmanidevi\"\"\""
      ],
      "metadata": {
        "id": "lvZ8DN06ONme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Results**"
      ],
      "metadata": {
        "id": "XrJAkpuj3Bqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Task(main_task,user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXgV-q6VcmvN",
        "outputId": "b5efc0a7-e146-44ba-f1c8-a702696749f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary is : \n",
            "Hugging Face, founded in 2016, has revolutionized Natural Language Processing (NLP). Its open-source contributions, particularly the Transformers library, have democratized AI. This library provides easy access to state-of-the-art pre-trained language models like BERT and GPT-3, enabling developers to leverage NLP's transformative potential for applications ranging from chatbots to sentiment analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Task(main_task2,user_input2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBoiX-KI26jL",
        "outputId": "b4a4e8b8-8e24-4cd4-e79a-0d4dbff9a53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 100 predictions new names are stored in /content/new_names.txt\n",
            "top 20 viable human name predictions are stored in /content/new_names_viable.txt\n",
            "\n",
            " \n",
            "\n",
            "Fine tuning on the following data :  ['avaneesh', 'sundararajan', 'rajeshwari', 'aditya', 'balasubramanian', 'umadevi', 'rukmanidevi']\n",
            "torch.Size([76, 16]) torch.Size([76])\n",
            "0  epochs complete\n",
            "Training loss : 1.7154 \n",
            "1  epochs complete\n",
            "Training loss : 1.5154 \n",
            "2  epochs complete\n",
            "Training loss : 1.5114 \n",
            "3  epochs complete\n",
            "Training loss : 1.5094 \n",
            "4  epochs complete\n",
            "Training loss : 1.5087 \n",
            "5  epochs complete\n",
            "Training loss : 1.5080 \n",
            "6  epochs complete\n",
            "Training loss : 1.5072 \n",
            "7  epochs complete\n",
            "Training loss : 1.5070 \n",
            "8  epochs complete\n",
            "Training loss : 1.5065 \n",
            "9  epochs complete\n",
            "Training loss : 1.5062 \n",
            "10  epochs complete\n",
            "Training loss : 1.5059 \n",
            "11  epochs complete\n",
            "Training loss : 1.5060 \n",
            "12  epochs complete\n",
            "Training loss : 1.5059 \n",
            "13  epochs complete\n",
            "Training loss : 1.5059 \n",
            "14  epochs complete\n",
            "Training loss : 1.5057 \n",
            "15  epochs complete\n",
            "Training loss : 1.5053 \n",
            "16  epochs complete\n",
            "Training loss : 1.5054 \n",
            "17  epochs complete\n",
            "Training loss : 1.5053 \n",
            "18  epochs complete\n",
            "Training loss : 1.5060 \n",
            "19  epochs complete\n",
            "Training loss : 1.5053 \n",
            "best model has been saved at '/content/finetune_model.pth'\n",
            "\n",
            " \n",
            "\n",
            "top 100 predicted new names after fine tuning are stored in /content/new_names_finetune.txt\n",
            "top 20 viable human name predictions after fine tuning are stored in /content/new_names_viable_finetune.txt\n"
          ]
        }
      ]
    }
  ]
}